{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c03947b4-1cd2-43c7-a9d9-4c0ca576979a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Required Imports #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721f94d7-ab82-4ba2-94ab-0ce22bf675a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install tensorflow_datasets --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b1c344f6-e8ab-47b6-a097-f989addd3aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.callbacks import LambdaCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46005e2-862d-445d-bfb9-37e415e113da",
   "metadata": {},
   "source": [
    "# ANN Model Manipulation #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6d666f-7544-47e5-a40c-61e9e5e1bd7a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Preprocessing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2539d68a-27ed-41b2-8cb7-62d62d718318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure your directories are clean for your train and test data!!\n",
    "try:\n",
    "    shutil.rmtree(\"Data/Split_Data/Test\")\n",
    "    shutil.rmtree(\"Data/Split_Data/Train\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "477a503e-9d88-4bf3-923e-d21b1f12ef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's make our train/test directories!!\n",
    "\n",
    "Path(\"Data/Split_Data/Test/No_Damage\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"Data/Split_Data/Test/Damaged\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "Path(\"Data/Split_Data/Train/No_Damage\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"Data/Split_Data/Train/Damaged\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Now We have to make sure we have a path for all the files.\n",
    "All_NoDmg = os.listdir('Data/OG_Data/no_damage')\n",
    "All_Dmg = os.listdir('Data/OG_Data/damage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "daaa336b-f3ee-44e3-b752-8d59da855200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train No_Damage image count:  5721\n",
      "Test No_Damage image count:  1431\n",
      "Length of overlap:  0\n",
      "\n",
      "\n",
      "Train No Damage image count:  11336\n",
      "Test No Damage image count:  2834\n",
      "Length of overlap:  0\n"
     ]
    }
   ],
   "source": [
    "# Time to split our data into the train and test folders!! Using 80/20!!\n",
    "\n",
    "Tr_NoDmg = random.sample(All_NoDmg, int(len(All_NoDmg)*0.8))\n",
    "print(\"Train No_Damage image count: \", len(Tr_NoDmg))\n",
    "Ts_NoDmg = [ p for p in All_NoDmg if p not in Tr_NoDmg]\n",
    "print(\"Test No_Damage image count: \", len(Ts_NoDmg))\n",
    "# ensure no overlap:\n",
    "overlap = [p for p in Tr_NoDmg if p in Ts_NoDmg]\n",
    "print(\"Length of overlap: \", len(overlap))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "Tr_Dmg = random.sample(All_Dmg, int(len(All_Dmg)*0.8))\n",
    "print(\"Train No Damage image count: \", len(Tr_Dmg))\n",
    "Ts_Dmg = [ p for p in All_Dmg if p not in Tr_Dmg]\n",
    "print(\"Test No Damage image count: \", len(Ts_Dmg))\n",
    "# ensure no overlap:\n",
    "overlap = [p for p in Tr_Dmg if p in Ts_Dmg]\n",
    "print(\"Length of overlap: \", len(overlap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "65810bab-b2ab-445e-9db7-bb375f3e4a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in Train/No_Damage:  5721\n",
      "Files in Test/No_Damage:  1431\n",
      "\n",
      "\n",
      "Files in Train/Damaged:  11336\n",
      "Files in Test/Damaged:  2834\n"
     ]
    }
   ],
   "source": [
    "# Make sure to actually put the data into the folders hehe\n",
    "\n",
    "for p in Ts_NoDmg:\n",
    "    shutil.copyfile(os.path.join('Data/OG_Data/no_damage', p), os.path.join('Data/Split_Data/Test/No_Damage', p) )\n",
    "for p in Tr_NoDmg:\n",
    "    shutil.copyfile(os.path.join('Data/OG_Data/no_damage', p), os.path.join('Data/Split_Data/Train/No_Damage', p) )\n",
    "\n",
    "for p in Ts_Dmg:\n",
    "    shutil.copyfile(os.path.join('Data/OG_Data/damage', p), os.path.join('Data/Split_Data/Test/Damaged', p) )\n",
    "for p in Tr_Dmg:\n",
    "    shutil.copyfile(os.path.join('Data/OG_Data/damage', p), os.path.join('Data/Split_Data/Train/Damaged', p) )\n",
    "\n",
    "# check counts:\n",
    "cpd_TrNoDmg = len(os.listdir(\"Data/Split_Data/Train/No_Damage\"))\n",
    "cpd_TsNoDmg = len(os.listdir(\"Data/Split_Data/Test/No_Damage\"))\n",
    "cpd_TrDmg = len(os.listdir(\"Data/Split_Data/Train/Damaged\"))\n",
    "cpd_TsDmg = len(os.listdir(\"Data/Split_Data/Test/Damaged\"))\n",
    "\n",
    "print(\"Files in Train/No_Damage: \", cpd_TrNoDmg)\n",
    "print(\"Files in Test/No_Damage: \", cpd_TsNoDmg)\n",
    "print(\"\\n\")\n",
    "print(\"Files in Train/Damaged: \", cpd_TrDmg)\n",
    "print(\"Files in Test/Damaged: \", cpd_TsDmg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8472ef94-ac8f-48bc-a632-26aa292f1668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data for no damaged shape: (5721,) & training damaged shape: (11336,)\n",
      "Testing data for no damaged shape: (1431,) & testing damaged shape: (2834,)\n"
     ]
    }
   ],
   "source": [
    "# We need them to be 1D arrays!!\n",
    "print(\"Training data for no damaged shape:\", TrNoDmg.shape, \"& training damaged shape:\", TrDmg.shape)\n",
    "print(\"Testing data for no damaged shape:\", TsNoDmg.shape, \"& testing damaged shape:\", TsDmg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6dfdaf-86ba-4fdd-9c16-4f253e04c554",
   "metadata": {},
   "source": [
    "## ANN Model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49819443-199d-4c62-b1af-d435289bbc56",
   "metadata": {},
   "source": [
    "For ANN Models we need to flatten the images, and with a bit of external investigation, I saw that the majority of the file I checked had a size of 128x128 pixels. So I am going to use that as my base!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b18152e0-7daa-4792-9d4e-b858391f1dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17057 files belonging to 2 classes.\n",
      "Using 13646 files for training.\n",
      "Using 3411 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = 'Data/Split_Data/Train'\n",
    "\n",
    "batch_size = 32\n",
    "# target image size\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "\n",
    "# note that subset=\"training\", \"validation\", \"both\", and dictates which dataset is returned\n",
    "train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "train_data_dir,\n",
    "validation_split=0.2,\n",
    "subset=\"both\",\n",
    "seed=224,\n",
    "image_size=(img_height, img_width),\n",
    "batch_size=batch_size\n",
    ")\n",
    "rescale = Rescaling(scale=1.0/255)\n",
    "train_rescale_ds = train_ds.map(lambda image,label:(rescale(image),label))\n",
    "val_rescale_ds = val_ds.map(lambda image,label:(rescale(image),label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "bea71c21-58b4-4303-b14e-8c4b9396ae67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4265 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_dir = 'Data/Split_Data/Test'\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "\n",
    "# note that subset=\"training\", \"validation\", \"both\", and dictates what is returned\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "test_data_dir,\n",
    "seed=224,\n",
    "image_size=(img_height, img_width),\n",
    ")\n",
    "\n",
    "rescale = Rescaling(scale=1.0/255)\n",
    "test_rescale_ds = test_ds.map(lambda image,label:(rescale(image),label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "32fc58d3-cb4d-4620-be5d-b3acd4162747",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bace41ba-dd7c-4981-baeb-19ebe8a9ed77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "427/427 [==============================] - 14s 31ms/step - loss: 0.8167 - accuracy: 0.6679 - val_loss: 0.6393 - val_accuracy: 0.6233\n",
      "Epoch 2/20\n",
      "427/427 [==============================] - 14s 31ms/step - loss: 0.5984 - accuracy: 0.7022 - val_loss: 0.5554 - val_accuracy: 0.7253\n",
      "Epoch 3/20\n",
      "427/427 [==============================] - 13s 30ms/step - loss: 0.5696 - accuracy: 0.7270 - val_loss: 0.6025 - val_accuracy: 0.6646\n",
      "Epoch 4/20\n",
      "427/427 [==============================] - 13s 30ms/step - loss: 0.5529 - accuracy: 0.7357 - val_loss: 0.5449 - val_accuracy: 0.7403\n",
      "Epoch 5/20\n",
      "427/427 [==============================] - 13s 31ms/step - loss: 0.5723 - accuracy: 0.7209 - val_loss: 0.5894 - val_accuracy: 0.7060\n",
      "Epoch 6/20\n",
      "427/427 [==============================] - 13s 30ms/step - loss: 0.5574 - accuracy: 0.7330 - val_loss: 0.5471 - val_accuracy: 0.7417\n",
      "Epoch 7/20\n",
      "427/427 [==============================] - 13s 30ms/step - loss: 0.5848 - accuracy: 0.6890 - val_loss: 0.6080 - val_accuracy: 0.6573\n",
      "Epoch 8/20\n",
      "427/427 [==============================] - 13s 30ms/step - loss: 0.6211 - accuracy: 0.6664 - val_loss: 0.6439 - val_accuracy: 0.6573\n",
      "Epoch 9/20\n",
      "427/427 [==============================] - 12s 29ms/step - loss: 0.6374 - accuracy: 0.6664 - val_loss: 0.6429 - val_accuracy: 0.6573\n",
      "Epoch 10/20\n",
      "427/427 [==============================] - 11s 26ms/step - loss: 0.6248 - accuracy: 0.6664 - val_loss: 0.6422 - val_accuracy: 0.6573\n",
      "Epoch 11/20\n",
      "427/427 [==============================] - 11s 26ms/step - loss: 0.6210 - accuracy: 0.6664 - val_loss: 0.6051 - val_accuracy: 0.6573\n",
      "Epoch 12/20\n",
      "427/427 [==============================] - 11s 27ms/step - loss: 0.6356 - accuracy: 0.6664 - val_loss: 0.6428 - val_accuracy: 0.6573\n",
      "Epoch 13/20\n",
      "427/427 [==============================] - 12s 27ms/step - loss: 0.6368 - accuracy: 0.6664 - val_loss: 0.6430 - val_accuracy: 0.6573\n",
      "Epoch 14/20\n",
      "427/427 [==============================] - 12s 28ms/step - loss: 0.6367 - accuracy: 0.6664 - val_loss: 0.6430 - val_accuracy: 0.6573\n",
      "Epoch 15/20\n",
      "427/427 [==============================] - 12s 28ms/step - loss: 0.6368 - accuracy: 0.6664 - val_loss: 0.6430 - val_accuracy: 0.6573\n",
      "Epoch 16/20\n",
      "427/427 [==============================] - 14s 32ms/step - loss: 0.6368 - accuracy: 0.6664 - val_loss: 0.6430 - val_accuracy: 0.6573\n",
      "Epoch 17/20\n",
      "427/427 [==============================] - 13s 31ms/step - loss: 0.6367 - accuracy: 0.6664 - val_loss: 0.6430 - val_accuracy: 0.6573\n",
      "Epoch 18/20\n",
      "427/427 [==============================] - 14s 33ms/step - loss: 0.6367 - accuracy: 0.6664 - val_loss: 0.6430 - val_accuracy: 0.6573\n",
      "Epoch 19/20\n",
      "427/427 [==============================] - 13s 31ms/step - loss: 0.6367 - accuracy: 0.6664 - val_loss: 0.6430 - val_accuracy: 0.6573\n",
      "Epoch 20/20\n",
      "427/427 [==============================] - 11s 27ms/step - loss: 0.6367 - accuracy: 0.6664 - val_loss: 0.6428 - val_accuracy: 0.6573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fce97466590>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_rescale_ds,batch_size=32,epochs=20,validation_data=val_rescale_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9a8cd463-65b0-40a8-aa26-8e026aec3804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6644783020019531"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_rescale_ds, verbose=0)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13c7591-fabd-49fb-b6ef-66104132d437",
   "metadata": {},
   "source": [
    "# CNN Model Manipulation #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5a429a-a4b5-4a27-8240-d4b64b13b733",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Preprocessing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0064f661-414e-4ffb-9058-a05565b40272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure your directories are clean for your train and test data!!\n",
    "try:\n",
    "    shutil.rmtree(\"Data/Split_Data/Test\")\n",
    "    shutil.rmtree(\"Data/Split_Data/Train\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e6c4886e-ebb3-4053-8966-4d4bad879536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's make our train/test directories!!\n",
    "\n",
    "Path(\"Data/Split_Data/Test/No_Damage\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"Data/Split_Data/Test/Damaged\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "Path(\"Data/Split_Data/Train/No_Damage\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"Data/Split_Data/Train/Damaged\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Now We have to make sure we have a path for all the files.\n",
    "All_NoDmg = os.listdir('Data/OG_Data/no_damage')\n",
    "All_Dmg = os.listdir('Data/OG_Data/damage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ba897419-612d-4264-a993-5d6a2064dd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train No_Damage image count:  5721\n",
      "Test No_Damage image count:  1431\n",
      "Length of overlap:  0\n",
      "\n",
      "\n",
      "Train No Damage image count:  11336\n",
      "Test No Damage image count:  2834\n",
      "Length of overlap:  0\n"
     ]
    }
   ],
   "source": [
    "# Time to split our data into the train and test folders!! Using 80/20!!\n",
    "\n",
    "Tr_NoDmg = random.sample(All_NoDmg, int(len(All_NoDmg)*0.8))\n",
    "print(\"Train No_Damage image count: \", len(Tr_NoDmg))\n",
    "Ts_NoDmg = [ p for p in All_NoDmg if p not in Tr_NoDmg]\n",
    "print(\"Test No_Damage image count: \", len(Ts_NoDmg))\n",
    "# ensure no overlap:\n",
    "overlap = [p for p in Tr_NoDmg if p in Ts_NoDmg]\n",
    "print(\"Length of overlap: \", len(overlap))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "Tr_Dmg = random.sample(All_Dmg, int(len(All_Dmg)*0.8))\n",
    "print(\"Train No Damage image count: \", len(Tr_Dmg))\n",
    "Ts_Dmg = [ p for p in All_Dmg if p not in Tr_Dmg]\n",
    "print(\"Test No Damage image count: \", len(Ts_Dmg))\n",
    "# ensure no overlap:\n",
    "overlap = [p for p in Tr_Dmg if p in Ts_Dmg]\n",
    "print(\"Length of overlap: \", len(overlap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a9b1ddad-595f-4a51-a453-814e99157e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in Train/No_Damage:  5721\n",
      "Files in Test/No_Damage:  1431\n",
      "\n",
      "\n",
      "Files in Train/Damaged:  11336\n",
      "Files in Test/Damaged:  2834\n"
     ]
    }
   ],
   "source": [
    "# Make sure to actually put the data into the folders hehe\n",
    "\n",
    "for p in Ts_NoDmg:\n",
    "    shutil.copyfile(os.path.join('Data/OG_Data/no_damage', p), os.path.join('Data/Split_Data/Test/No_Damage', p) )\n",
    "for p in Tr_NoDmg:\n",
    "    shutil.copyfile(os.path.join('Data/OG_Data/no_damage', p), os.path.join('Data/Split_Data/Train/No_Damage', p) )\n",
    "\n",
    "for p in Ts_Dmg:\n",
    "    shutil.copyfile(os.path.join('Data/OG_Data/damage', p), os.path.join('Data/Split_Data/Test/Damaged', p) )\n",
    "for p in Tr_Dmg:\n",
    "    shutil.copyfile(os.path.join('Data/OG_Data/damage', p), os.path.join('Data/Split_Data/Train/Damaged', p) )\n",
    "\n",
    "# check counts:\n",
    "cpd_TrNoDmg = len(os.listdir(\"Data/Split_Data/Train/No_Damage\"))\n",
    "cpd_TsNoDmg = len(os.listdir(\"Data/Split_Data/Test/No_Damage\"))\n",
    "cpd_TrDmg = len(os.listdir(\"Data/Split_Data/Train/Damaged\"))\n",
    "cpd_TsDmg = len(os.listdir(\"Data/Split_Data/Test/Damaged\"))\n",
    "\n",
    "print(\"Files in Train/No_Damage: \", cpd_TrNoDmg)\n",
    "print(\"Files in Test/No_Damage: \", cpd_TsNoDmg)\n",
    "print(\"\\n\")\n",
    "print(\"Files in Train/Damaged: \", cpd_TrDmg)\n",
    "print(\"Files in Test/Damaged: \", cpd_TsDmg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9ac472-e880-4816-9da9-c0846ccf20da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Lenet-5 CNN Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "db4b6237-e674-4985-9866-9f01d4d29a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17057 files belonging to 2 classes.\n",
      "Using 13646 files for training.\n",
      "Using 3411 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = 'Data/Split_Data/Train'\n",
    "\n",
    "batch_size = 32\n",
    "# target image size\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "\n",
    "# note that subset=\"training\", \"validation\", \"both\", and dictates which dataset is returned\n",
    "train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "train_data_dir,\n",
    "validation_split=0.2,\n",
    "subset=\"both\",\n",
    "seed=224,\n",
    "image_size=(img_height, img_width),\n",
    "batch_size=batch_size\n",
    ")\n",
    "rescale = Rescaling(scale=1.0/255)\n",
    "train_rescale_ds = train_ds.map(lambda image,label:(rescale(image),label))\n",
    "val_rescale_ds = val_ds.map(lambda image,label:(rescale(image),label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "42302267-81e5-4a7d-85e0-65fc90981763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4265 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_dir = 'Data/Split_Data/Test'\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "\n",
    "# note that subset=\"training\", \"validation\", \"both\", and dictates what is returned\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "test_data_dir,\n",
    "seed=224,\n",
    "image_size=(img_height, img_width),\n",
    ")\n",
    "\n",
    "# approach 1: manually rescale data --\n",
    "rescale = Rescaling(scale=1.0/255)\n",
    "test_rescale_ds = test_ds.map(lambda image,label:(rescale(image),label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "652d75cc-ad99-4e3c-af7a-94577b99e403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_1 (Conv2D)           (None, 126, 126, 6)       168       \n",
      "                                                                 \n",
      " average_pooling2d_1 (Avera  (None, 63, 63, 6)         0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 63, 63, 64)        3520      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 61, 61, 16)        9232      \n",
      "                                                                 \n",
      " average_pooling2d_2 (Avera  (None, 30, 30, 16)        0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " flatten_12 (Flatten)        (None, 14400)             0         \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 120)               1728120   \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 3)                 255       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1751459 (6.68 MB)\n",
      "Trainable params: 1751459 (6.68 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_size=128*128\n",
    "\n",
    "model_lenet5 = models.Sequential()\n",
    "\n",
    "# Layer 1: Convolutional layer with 6 filters of size 3x3, followed by average pooling\n",
    "model_lenet5.add(layers.Conv2D(6, kernel_size=(3, 3), activation='relu', input_shape=(128,128,3)))\n",
    "model_lenet5.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "model_lenet5.add(layers.Conv2D(64, (3, 3), activation='relu', padding=\"same\", input_shape=(128,128,3)))\n",
    "\n",
    "# Layer 2: Convolutional layer with 16 filters of size 3x3, followed by average pooling\n",
    "model_lenet5.add(layers.Conv2D(16, kernel_size=(3, 3), activation='relu'))\n",
    "model_lenet5.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the feature maps to feed into fully connected layers\n",
    "model_lenet5.add(layers.Flatten())\n",
    "\n",
    "# Layer 3: Fully connected layer with 120 neurons\n",
    "model_lenet5.add(layers.Dense(120, activation='relu'))\n",
    "\n",
    "# Layer 4: Fully connected layer with 84 neurons\n",
    "model_lenet5.add(layers.Dense(84, activation='relu'))\n",
    "\n",
    "# Output layer: Fully connected layer with num_classes neurons (e.g., 3 )\n",
    "model_lenet5.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model_lenet5.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Generating the summary of the model\n",
    "model_lenet5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "0b8f3de4-4df0-4a9b-aeef-244090b1a297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "427/427 [==============================] - 59s 138ms/step - loss: 0.6430 - accuracy: 0.6672 - val_loss: 0.5962 - val_accuracy: 0.6936\n",
      "Epoch 2/20\n",
      "427/427 [==============================] - 50s 116ms/step - loss: 0.5263 - accuracy: 0.7486 - val_loss: 0.4575 - val_accuracy: 0.8156\n",
      "Epoch 3/20\n",
      "427/427 [==============================] - 56s 132ms/step - loss: 0.4451 - accuracy: 0.8022 - val_loss: 0.5033 - val_accuracy: 0.7508\n",
      "Epoch 4/20\n",
      "427/427 [==============================] - 62s 144ms/step - loss: 0.3953 - accuracy: 0.8276 - val_loss: 0.4418 - val_accuracy: 0.7895\n",
      "Epoch 5/20\n",
      "427/427 [==============================] - 63s 147ms/step - loss: 0.3465 - accuracy: 0.8486 - val_loss: 0.3332 - val_accuracy: 0.8602\n",
      "Epoch 6/20\n",
      "427/427 [==============================] - 53s 123ms/step - loss: 0.3143 - accuracy: 0.8654 - val_loss: 0.3314 - val_accuracy: 0.8484\n",
      "Epoch 7/20\n",
      "427/427 [==============================] - 61s 144ms/step - loss: 0.2848 - accuracy: 0.8778 - val_loss: 0.3195 - val_accuracy: 0.8619\n",
      "Epoch 8/20\n",
      "427/427 [==============================] - 62s 145ms/step - loss: 0.2662 - accuracy: 0.8869 - val_loss: 0.4115 - val_accuracy: 0.8253\n",
      "Epoch 9/20\n",
      "427/427 [==============================] - 62s 145ms/step - loss: 0.2414 - accuracy: 0.9016 - val_loss: 0.2420 - val_accuracy: 0.9035\n",
      "Epoch 10/20\n",
      "427/427 [==============================] - 57s 133ms/step - loss: 0.2243 - accuracy: 0.9076 - val_loss: 0.2096 - val_accuracy: 0.9197\n",
      "Epoch 11/20\n",
      "427/427 [==============================] - 60s 142ms/step - loss: 0.2071 - accuracy: 0.9157 - val_loss: 0.2195 - val_accuracy: 0.9115\n",
      "Epoch 12/20\n",
      "427/427 [==============================] - 61s 142ms/step - loss: 0.1873 - accuracy: 0.9247 - val_loss: 0.2029 - val_accuracy: 0.9214\n",
      "Epoch 13/20\n",
      "427/427 [==============================] - 59s 137ms/step - loss: 0.1816 - accuracy: 0.9258 - val_loss: 0.3256 - val_accuracy: 0.8578\n",
      "Epoch 14/20\n",
      "427/427 [==============================] - 56s 130ms/step - loss: 0.1683 - accuracy: 0.9324 - val_loss: 0.2111 - val_accuracy: 0.9144\n",
      "Epoch 15/20\n",
      "427/427 [==============================] - 58s 136ms/step - loss: 0.1558 - accuracy: 0.9392 - val_loss: 0.2519 - val_accuracy: 0.9000\n",
      "Epoch 16/20\n",
      "427/427 [==============================] - 57s 134ms/step - loss: 0.1490 - accuracy: 0.9437 - val_loss: 0.1747 - val_accuracy: 0.9346\n",
      "Epoch 17/20\n",
      "427/427 [==============================] - 60s 141ms/step - loss: 0.1404 - accuracy: 0.9444 - val_loss: 0.1936 - val_accuracy: 0.9267\n",
      "Epoch 18/20\n",
      "427/427 [==============================] - 61s 142ms/step - loss: 0.1336 - accuracy: 0.9490 - val_loss: 0.1621 - val_accuracy: 0.9411\n",
      "Epoch 19/20\n",
      "427/427 [==============================] - 57s 134ms/step - loss: 0.1235 - accuracy: 0.9537 - val_loss: 0.1514 - val_accuracy: 0.9455\n",
      "Epoch 20/20\n",
      "427/427 [==============================] - 56s 131ms/step - loss: 0.1194 - accuracy: 0.9552 - val_loss: 0.1466 - val_accuracy: 0.9431\n"
     ]
    }
   ],
   "source": [
    "#fit the model from image generator\n",
    "history = model_lenet5.fit(\n",
    "            train_rescale_ds,\n",
    "            batch_size=32,\n",
    "            epochs=20,\n",
    "            validation_data=val_rescale_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e6d742f1-d8f2-4844-8a5c-47851b295f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9470105767250061"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_accuracy = model_lenet5.evaluate(test_rescale_ds, verbose=0)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30d5fea-976a-4562-b94f-5cebbab18bf6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Alternate Lenet-5 CNN Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6b04305a-3f0a-4a53-9361-667a4b7a8f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 126, 126, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 63, 63, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 61, 61, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 30, 30, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 28, 28, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 14, 14, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 12, 12, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 6, 6, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_13 (Flatten)        (None, 4608)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 512)               2359808   \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2601666 (9.92 MB)\n",
      "Trainable params: 2601666 (9.92 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_size=128*128\n",
    "\n",
    "model_altL5 = models.Sequential()\n",
    "\n",
    "# Layer 1: Convolutional layer with 32 filters of size 3x3, followed by average pooling\n",
    "model_altL5.add(layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(128,128,3)))\n",
    "model_altL5.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 2: Convolutional layer with 64 filters of size 3x3, followed by average pooling\n",
    "model_altL5.add(layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model_altL5.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 3: Convolutional layer with 128 filters of size 3x3, followed by average pooling\n",
    "model_altL5.add(layers.Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(128,128,3)))\n",
    "model_altL5.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 4: Convolutional layer with 32 filters of size 3x3, followed by average pooling\n",
    "model_altL5.add(layers.Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(128,128,3)))\n",
    "model_altL5.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the feature maps to feed into fully connected layers\n",
    "model_altL5.add(layers.Flatten())\n",
    "\n",
    "# Adding dropout prevents overfitting\n",
    "model_altL5.add(layers.Dropout(0.2))\n",
    "\n",
    "# Layer 4: Fully connected layer with 84 neurons\n",
    "model_altL5.add(layers.Dense(512, activation='relu'))\n",
    "\n",
    "# Output layer: Fully connected layer with num_classes neurons (e.g., 3 )\n",
    "model_altL5.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model_altL5.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Generating the summary of the model\n",
    "model_altL5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "ca0ef438-dfc3-45e0-bb7a-587d1749211b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "427/427 [==============================] - 89s 206ms/step - loss: 0.4915 - accuracy: 0.7716 - val_loss: 0.7587 - val_accuracy: 0.7060\n",
      "Epoch 2/20\n",
      "427/427 [==============================] - 91s 213ms/step - loss: 0.3411 - accuracy: 0.8570 - val_loss: 0.3173 - val_accuracy: 0.8716\n",
      "Epoch 3/20\n",
      "427/427 [==============================] - 90s 210ms/step - loss: 0.2434 - accuracy: 0.9044 - val_loss: 0.4690 - val_accuracy: 0.7804\n",
      "Epoch 4/20\n",
      "427/427 [==============================] - 88s 206ms/step - loss: 0.1869 - accuracy: 0.9261 - val_loss: 0.2277 - val_accuracy: 0.9088\n",
      "Epoch 5/20\n",
      "427/427 [==============================] - 86s 201ms/step - loss: 0.1498 - accuracy: 0.9419 - val_loss: 0.1364 - val_accuracy: 0.9540\n",
      "Epoch 6/20\n",
      "427/427 [==============================] - 87s 204ms/step - loss: 0.1282 - accuracy: 0.9504 - val_loss: 0.2018 - val_accuracy: 0.9170\n",
      "Epoch 7/20\n",
      "427/427 [==============================] - 89s 209ms/step - loss: 0.1160 - accuracy: 0.9538 - val_loss: 0.1112 - val_accuracy: 0.9566\n",
      "Epoch 8/20\n",
      "427/427 [==============================] - 84s 197ms/step - loss: 0.1028 - accuracy: 0.9598 - val_loss: 0.1118 - val_accuracy: 0.9543\n",
      "Epoch 9/20\n",
      "427/427 [==============================] - 84s 198ms/step - loss: 0.0958 - accuracy: 0.9626 - val_loss: 0.0995 - val_accuracy: 0.9610\n",
      "Epoch 10/20\n",
      "427/427 [==============================] - 83s 193ms/step - loss: 0.0903 - accuracy: 0.9628 - val_loss: 0.2301 - val_accuracy: 0.9138\n",
      "Epoch 11/20\n",
      "427/427 [==============================] - 84s 196ms/step - loss: 0.0833 - accuracy: 0.9666 - val_loss: 0.0970 - val_accuracy: 0.9613\n",
      "Epoch 12/20\n",
      "427/427 [==============================] - 82s 191ms/step - loss: 0.0784 - accuracy: 0.9687 - val_loss: 0.0809 - val_accuracy: 0.9689\n",
      "Epoch 13/20\n",
      "427/427 [==============================] - 85s 198ms/step - loss: 0.0725 - accuracy: 0.9737 - val_loss: 0.0971 - val_accuracy: 0.9636\n",
      "Epoch 14/20\n",
      "427/427 [==============================] - 84s 197ms/step - loss: 0.0687 - accuracy: 0.9750 - val_loss: 0.1264 - val_accuracy: 0.9554\n",
      "Epoch 15/20\n",
      "427/427 [==============================] - 83s 193ms/step - loss: 0.0650 - accuracy: 0.9755 - val_loss: 0.0702 - val_accuracy: 0.9754\n",
      "Epoch 16/20\n",
      "427/427 [==============================] - 83s 195ms/step - loss: 0.0580 - accuracy: 0.9777 - val_loss: 0.3811 - val_accuracy: 0.8528\n",
      "Epoch 17/20\n",
      "427/427 [==============================] - 83s 194ms/step - loss: 0.0576 - accuracy: 0.9781 - val_loss: 0.0897 - val_accuracy: 0.9680\n",
      "Epoch 18/20\n",
      "427/427 [==============================] - 82s 193ms/step - loss: 0.0525 - accuracy: 0.9803 - val_loss: 0.1057 - val_accuracy: 0.9592\n",
      "Epoch 19/20\n",
      "427/427 [==============================] - 80s 188ms/step - loss: 0.0511 - accuracy: 0.9816 - val_loss: 0.0712 - val_accuracy: 0.9751\n",
      "Epoch 20/20\n",
      "427/427 [==============================] - 82s 191ms/step - loss: 0.0467 - accuracy: 0.9848 - val_loss: 0.0757 - val_accuracy: 0.9727\n"
     ]
    }
   ],
   "source": [
    "#fit the model from image generator\n",
    "history = model_altL5.fit(\n",
    "            train_rescale_ds,\n",
    "            batch_size=32,\n",
    "            epochs=20,\n",
    "            validation_data=val_rescale_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "df0bffc5-b149-42e2-b0f2-ecd4923b4eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96858149766922"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_accuracy = model_altL5.evaluate(test_rescale_ds, verbose=0)\n",
    "test_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
