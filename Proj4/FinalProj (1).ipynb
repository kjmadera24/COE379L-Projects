{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20443a7d-b285-4fdf-9703-c525acfee469",
   "metadata": {},
   "source": [
    "# Required Imports #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18876195-e979-485a-814f-f359d74db14b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.9.4-py3-none-any.whl (5.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.11/site-packages (from tensorflow_datasets) (2.0.0)\n",
      "Collecting click\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m164.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dm-tree\n",
      "  Downloading dm_tree-0.1.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.8/152.8 kB\u001b[0m \u001b[31m186.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting etils[enp,epath,etree]>=0.9.0\n",
      "  Downloading etils-1.8.0-py3-none-any.whl (156 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.1/156.1 kB\u001b[0m \u001b[31m205.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/site-packages (from tensorflow_datasets) (1.26.2)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/site-packages (from tensorflow_datasets) (4.23.4)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/site-packages (from tensorflow_datasets) (5.9.7)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/site-packages (from tensorflow_datasets) (2.31.0)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.15.0-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/site-packages (from tensorflow_datasets) (2.4.0)\n",
      "Collecting toml\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m258.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.11/site-packages (from tensorflow_datasets) (1.14.1)\n",
      "Collecting array-record>=0.5.0\n",
      "  Downloading array_record-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m155.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m287.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting importlib_resources\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (4.9.0)\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.11/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (2023.11.17)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Collecting googleapis-common-protos<2,>=1.56.4\n",
      "  Downloading googleapis_common_protos-1.63.0-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.1/229.1 kB\u001b[0m \u001b[31m282.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protobuf>=3.20\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m292.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21483 sha256=71ca71ea5742ff4e4031ce1a4d7c4098c61c0e8f4250e52e3e70e583d3ea9956\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fn3banf6/wheels/74/05/89/d0909dd6ebad0a26f2b4dcb2499b1d65999c5b6ed416be7f85\n",
      "Successfully built promise\n",
      "Installing collected packages: dm-tree, tqdm, toml, protobuf, promise, importlib_resources, fsspec, etils, click, googleapis-common-protos, tensorflow-metadata, array-record, tensorflow_datasets\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script tfds is installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed array-record-0.5.1 click-8.1.7 dm-tree-0.1.8 etils-1.8.0 fsspec-2024.3.1 googleapis-common-protos-1.63.0 importlib_resources-6.4.0 promise-2.3 protobuf-4.25.3 tensorflow-metadata-1.15.0 tensorflow_datasets-4.9.4 toml-0.10.2 tqdm-4.66.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow_datasets --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9374de2-5edb-4797-8050-1263f249a81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-03 17:12:30.972957: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-03 17:12:31.028716: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-03 17:12:31.028790: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-03 17:12:31.033100: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-03 17:12:31.050116: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-03 17:12:31.051464: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-03 17:12:37.071318: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow_datasets.datasets import stanford_dogs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e282b7f-5559-4049-a6c2-f124fa9992a3",
   "metadata": {},
   "source": [
    "# Data Preprocessing #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ee1352d-e773-4efa-b3ce-b00c69ec9e60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-03 17:12:42.723683: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 778.12 MiB (download: 778.12 MiB, generated: Unknown size, total: 778.12 MiB) to /root/tensorflow_datasets/stanford_dogs/0.2.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9937a6fb7f644b338f2e8e92f8219c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0d0eb37aae458184b3e6eacf1cf8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f780d11cbb845eba5a71b433f9c8e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bebdfac35f548aeb2846a37f2a29cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16310d5ba9804498bb6e5d07ce6ac12b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /root/tensorflow_datasets/stanford_dogs/0.2.0.incompleteT68AGW/stanford_dogs-train.tfrecord*...:   0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/8580 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /root/tensorflow_datasets/stanford_dogs/0.2.0.incompleteT68AGW/stanford_dogs-test.tfrecord*...:   0%…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset stanford_dogs downloaded and prepared to /root/tensorflow_datasets/stanford_dogs/0.2.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "(dogs_train, dogs_test), ds_info = tfds.load(\n",
    "    'stanford_dogs',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcbc6be-a0dd-4ec0-9ceb-4629b6f15c19",
   "metadata": {},
   "source": [
    "Just to get a feel for the data I'm dealing with!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef486da2-6cb7-454c-a472-a7c49da223d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>\n",
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))> \n",
      "\n",
      "tfds.core.DatasetInfo(\n",
      "    name='stanford_dogs',\n",
      "    full_name='stanford_dogs/0.2.0',\n",
      "    description=\"\"\"\n",
      "    The Stanford Dogs dataset contains images of 120 breeds of dogs from around the\n",
      "    world. This dataset has been built using images and annotation from ImageNet for\n",
      "    the task of fine-grained image categorization. There are 20,580 images, out of\n",
      "    which 12,000 are used for training and 8580 for testing. Class labels and\n",
      "    bounding box annotations are provided for all the 12,000 images.\n",
      "    \"\"\",\n",
      "    homepage='http://vision.stanford.edu/aditya86/ImageNetDogs/main.html',\n",
      "    data_dir=PosixGPath('/tmp/tmpzi469120tfds'),\n",
      "    file_format=tfrecord,\n",
      "    download_size=778.12 MiB,\n",
      "    dataset_size=744.72 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(None, None, 3), dtype=uint8),\n",
      "        'image/filename': Text(shape=(), dtype=string),\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=120),\n",
      "        'objects': Sequence({\n",
      "            'bbox': BBoxFeature(shape=(4,), dtype=float32),\n",
      "        }),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=8580, num_shards=4>,\n",
      "        'train': <SplitInfo num_examples=12000, num_shards=4>,\n",
      "    },\n",
      "    citation=\"\"\"@inproceedings{KhoslaYaoJayadevaprakashFeiFei_FGVC2011,\n",
      "    author = \"Aditya Khosla and Nityananda Jayadevaprakash and Bangpeng Yao and\n",
      "              Li Fei-Fei\",\n",
      "    title = \"Novel Dataset for Fine-Grained Image Categorization\",\n",
      "    booktitle = \"First Workshop on Fine-Grained Visual Categorization,\n",
      "                 IEEE Conference on Computer Vision and Pattern Recognition\",\n",
      "    year = \"2011\",\n",
      "    month = \"June\",\n",
      "    address = \"Colorado Springs, CO\",\n",
      "    }\n",
      "    @inproceedings{imagenet_cvpr09,\n",
      "            AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and\n",
      "                      Li, K. and Fei-Fei, L.},\n",
      "            TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},\n",
      "            BOOKTITLE = {CVPR09},\n",
      "            YEAR = {2009},\n",
      "            BIBSOURCE = \"http://www.image-net.org/papers/imagenet_cvpr09.bib\"}\"\"\",\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(dogs_train)\n",
    "print(dogs_test, '\\n')\n",
    "print(ds_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35107ce4-4ad3-4c87-b4a8-a75476e2e654",
   "metadata": {},
   "source": [
    "## Preprocessing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe82722-5145-44bd-a133-38bf3c1ba851",
   "metadata": {},
   "source": [
    "These functions are greatly appreciated by @aribiswas, they created a machine learning classifier using this same dataset and providing the code to replicate their results!! https://github.com/aribiswas/stanford-dogs-classifier/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00a8632b-b75a-47ef-a609-f2c37642da3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    Load stanford_dogs tensorflow dataset.\n",
    "    :return:\n",
    "    ds_train (tf.data.DataSet) The requested training dataset.\n",
    "    ds_test (tf.data.DataSet) The requested test dataset.\n",
    "    ds_info (tfds.core.DatasetInfo) The requested dataset info.\n",
    "    \"\"\"\n",
    "    (ds_train, ds_test), ds_info = tfds.load('stanford_dogs',\n",
    "                                             split=['train', 'test'],\n",
    "                                             shuffle_files=True,\n",
    "                                             as_supervised=False,\n",
    "                                             with_info=True,\n",
    "                                             data_dir='data/tfds')\n",
    "    return ds_train, ds_test, ds_info\n",
    "\n",
    "\n",
    "def preprocess(data, image_size, num_labels, cast=True, resize=True, normalize=True, one_hot=True):\n",
    "    \"\"\"\n",
    "    Process an image.\n",
    "    :param data: Tensorflow datset containing an image and label.\n",
    "    :param image_size: Size of the image. Images may be resized to this size. E.g. (224, 224)\n",
    "    :param num_labels: Number of labels for prediction.\n",
    "    :param cast: Flag for casting to float32. True or False.\n",
    "    :param resize: Flag for resizing the image. True or False.\n",
    "    :param normalize: Flag for normalizing the image pixel values from 0-1. True or False.\n",
    "    :param one_hot: Flag for one hot encoding the labels. True or False.\n",
    "    :return: Processed image and encoded label.\n",
    "    \"\"\"\n",
    "    # processed_image = tf.keras.applications.resnet.preprocess_input(data['image'])\n",
    "    processed_image = data['image']\n",
    "    label = data['label']\n",
    "    if cast:\n",
    "        processed_image = tf.cast(processed_image, tf.float32)\n",
    "    if resize:\n",
    "        processed_image = tf.image.resize(processed_image, image_size, method='nearest')\n",
    "    if normalize:\n",
    "        processed_image = processed_image / 255.\n",
    "    if one_hot:\n",
    "        label = tf.one_hot(label, num_labels)\n",
    "    return processed_image, label\n",
    "\n",
    "\n",
    "def prepare(dataset, image_shape, num_classes, batch_size=None):\n",
    "    \"\"\"\n",
    "    Prepare an input pipeline for training a dataset.\n",
    "    :param dataset: The dataset containing training data.\n",
    "    :param image_shape: A common shape of the input image. Images with different sizes will be resized. E.g. (80, 80, 3)\n",
    "    :param num_classes: Number of prediction classes.\n",
    "    :param batch_size: Batch size for training.\n",
    "    :return: Prepared dataset.\n",
    "    \"\"\"\n",
    "    dataset = dataset.map(lambda x: preprocess(x, image_shape[0:-1], num_classes),\n",
    "                          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.cache()\n",
    "    if batch_size is not None:\n",
    "        dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "407cfc6c-a4a8-432e-b082-51a725dcfc53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_shape = (224, 224, 3)\n",
    "num_breeds = 120\n",
    "\n",
    "ds_train, ds_test, ds_info = load_dataset()\n",
    "train_batches = prepare(ds_train, input_shape, num_breeds, batch_size=32)\n",
    "test_batches = prepare(ds_test, input_shape, num_breeds,  batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ffd4af-4c1c-4f01-b5c8-b3dde35213b9",
   "metadata": {},
   "source": [
    "# Lecture CNN Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae84aaf-09ef-456b-a7bd-1cc3a5998997",
   "metadata": {},
   "source": [
    "## Lenet-5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c793ff0-064d-4e50-a258-f2f498d697d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 222, 222, 6)       168       \n",
      "                                                                 \n",
      " average_pooling2d (Average  (None, 111, 111, 6)       0         \n",
      " Pooling2D)                                                      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 111, 111, 64)      3520      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 109, 109, 16)      9232      \n",
      "                                                                 \n",
      " average_pooling2d_1 (Avera  (None, 54, 54, 16)        0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 46656)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 120)               5598840   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 120)               10200     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5632124 (21.48 MB)\n",
      "Trainable params: 5632124 (21.48 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "breeds = 120\n",
    "\n",
    "model_lenet5 = models.Sequential()\n",
    "\n",
    "# Layer 1: Convolutional layer with 6 filters of size 3x3, followed by average pooling\n",
    "model_lenet5.add(layers.Conv2D(6, kernel_size=(3, 3), activation='relu', input_shape=(224,224,3)))\n",
    "model_lenet5.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "model_lenet5.add(layers.Conv2D(64, (3, 3), activation='relu', padding=\"same\", input_shape=(224,224,3)))\n",
    "\n",
    "# Layer 2: Convolutional layer with 16 filters of size 3x3, followed by average pooling\n",
    "model_lenet5.add(layers.Conv2D(16, kernel_size=(3, 3), activation='relu'))\n",
    "model_lenet5.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the feature maps to feed into fully connected layers\n",
    "model_lenet5.add(layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "# Layer 3: Fully connected layer with 120 neurons\n",
    "model_lenet5.add(layers.Dense(120, activation='relu'))\n",
    "\n",
    "# Layer 4: Fully connected layer with 84 neurons\n",
    "model_lenet5.add(layers.Dense(84, activation='relu'))\n",
    "\n",
    "# Output layer: Fully connected layer with num_classes neurons (e.g., 3 )\n",
    "model_lenet5.add(layers.Dense(120, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model_lenet5.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Generating the summary of the model\n",
    "model_lenet5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86eeb775-214e-4767-ad27-739e4416bc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 240s 637ms/step - loss: 4.7826 - accuracy: 0.0093 - val_loss: 4.7575 - val_accuracy: 0.0147\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 213s 568ms/step - loss: 4.7215 - accuracy: 0.0181 - val_loss: 4.6721 - val_accuracy: 0.0211\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 217s 580ms/step - loss: 4.5988 - accuracy: 0.0292 - val_loss: 4.5627 - val_accuracy: 0.0347\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 207s 551ms/step - loss: 4.4554 - accuracy: 0.0411 - val_loss: 4.4857 - val_accuracy: 0.0369\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 204s 544ms/step - loss: 4.3286 - accuracy: 0.0518 - val_loss: 4.4263 - val_accuracy: 0.0400\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 203s 542ms/step - loss: 4.2133 - accuracy: 0.0687 - val_loss: 4.4126 - val_accuracy: 0.0438\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 203s 541ms/step - loss: 4.0845 - accuracy: 0.0894 - val_loss: 4.4034 - val_accuracy: 0.0479\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 192s 513ms/step - loss: 3.9246 - accuracy: 0.1168 - val_loss: 4.4321 - val_accuracy: 0.0494\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 197s 526ms/step - loss: 3.7258 - accuracy: 0.1563 - val_loss: 4.5072 - val_accuracy: 0.0492\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 201s 535ms/step - loss: 3.4812 - accuracy: 0.2024 - val_loss: 4.6224 - val_accuracy: 0.0520\n"
     ]
    }
   ],
   "source": [
    "#fit the model from image generator\n",
    "history = model_lenet5.fit(\n",
    "            train_batches,\n",
    "            batch_size=32,\n",
    "            epochs=10,\n",
    "            validation_data=test_batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16dfccb0-88cc-45db-a2d2-beb9caed14f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22608333826065063"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_accuracy = model_lenet5.evaluate(train_batches, verbose=0)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea9b556-f14d-45df-b425-b9a7971c9a42",
   "metadata": {},
   "source": [
    "This is bad bad, even for only being 10 epochs haha.... I am going to try other CNN Models and see what happens :))\n",
    "I didnt expect such a low accuracy though, I assume its because there's more classification labels rather than 2 so It makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c163ad5-a217-4167-84fd-401e9809ac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lenet5.save('models/Lenet5.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1662f910-72ac-4d28-8b10-4683fbf9f541",
   "metadata": {},
   "source": [
    "## Alternate Lenet-5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c951012-0675-4b02-b346-6d0f972a94d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 111, 111, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 54, 54, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 52, 52, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 26, 26, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 24, 24, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 12, 12, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 18432)             0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 18432)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               9437696   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 120)               61560     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9740088 (37.16 MB)\n",
      "Trainable params: 9740088 (37.16 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_altL5 = models.Sequential()\n",
    "\n",
    "# Layer 1: Convolutional layer with 32 filters of size 3x3, followed by average pooling\n",
    "model_altL5.add(layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(224,224,3)))\n",
    "model_altL5.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 2: Convolutional layer with 64 filters of size 3x3, followed by average pooling\n",
    "model_altL5.add(layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model_altL5.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 3: Convolutional layer with 128 filters of size 3x3, followed by average pooling\n",
    "model_altL5.add(layers.Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(224,224,3)))\n",
    "model_altL5.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 4: Convolutional layer with 32 filters of size 3x3, followed by average pooling\n",
    "model_altL5.add(layers.Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(224,224,3)))\n",
    "model_altL5.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the feature maps to feed into fully connected layers\n",
    "model_altL5.add(layers.Flatten())\n",
    "\n",
    "# Adding dropout prevents overfitting\n",
    "model_altL5.add(layers.Dropout(0.2))\n",
    "\n",
    "# Layer 4: Fully connected layer with 84 neurons\n",
    "model_altL5.add(layers.Dense(512, activation='relu'))\n",
    "\n",
    "# Output layer: Fully connected layer with num_classes neurons (e.g., 3 )\n",
    "model_altL5.add(layers.Dense(120, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model_altL5.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Generating the summary of the model\n",
    "model_altL5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "208b71a7-411f-4eb1-b622-5142b409e188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 317s 843ms/step - loss: 4.7811 - accuracy: 0.0119 - val_loss: 4.7212 - val_accuracy: 0.0232\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 313s 835ms/step - loss: 4.5418 - accuracy: 0.0332 - val_loss: 4.4133 - val_accuracy: 0.0495\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 325s 867ms/step - loss: 4.2473 - accuracy: 0.0659 - val_loss: 4.3066 - val_accuracy: 0.0682\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 325s 866ms/step - loss: 3.9413 - accuracy: 0.1150 - val_loss: 4.2887 - val_accuracy: 0.0791\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 326s 870ms/step - loss: 3.5663 - accuracy: 0.1796 - val_loss: 4.3982 - val_accuracy: 0.0845\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 325s 868ms/step - loss: 3.0944 - accuracy: 0.2723 - val_loss: 4.6762 - val_accuracy: 0.0888\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 324s 864ms/step - loss: 2.5062 - accuracy: 0.3940 - val_loss: 5.1577 - val_accuracy: 0.0893\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 323s 861ms/step - loss: 1.8389 - accuracy: 0.5421 - val_loss: 5.9267 - val_accuracy: 0.0845\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 323s 861ms/step - loss: 1.1975 - accuracy: 0.6948 - val_loss: 6.8427 - val_accuracy: 0.0839\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 320s 853ms/step - loss: 0.7047 - accuracy: 0.8109 - val_loss: 8.2787 - val_accuracy: 0.0774\n"
     ]
    }
   ],
   "source": [
    "#fit the model from image generator\n",
    "history = model_altL5.fit(\n",
    "            train_batches,\n",
    "            batch_size=32,\n",
    "            epochs=10,\n",
    "            validation_data=test_batches\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "155342d7-619f-4eb5-a87c-b4e2ebc6cb01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8138333559036255"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_accuracy = model_altL5.evaluate(train_batches, verbose=0)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9feab95-e9cf-4fa0-8ada-1aa0e6d2b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_altL5.save('models/AltLenet5.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0f803f-6c5e-4e42-b67f-10ff9e78f3b2",
   "metadata": {},
   "source": [
    "# Resource Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46b499a-5297-4cf7-a854-6a60604da582",
   "metadata": {},
   "source": [
    "Just to compare the models used in lecture to some of the ones Aribiswas used!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e13c4e-2a64-47cd-b579-53dca0876fb4",
   "metadata": {},
   "source": [
    "## MobileNetV2 Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f511ff3-a833-4fbd-888b-8367e2deff48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9406464/9406464 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(input_shape=(224,224,3),\n",
    "                                                   include_top=False,\n",
    "                                                   weights='imagenet')\n",
    "base_model.trainable = False\n",
    "net = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(120, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10bcf22e-7dbf-44ec-9acf-09dfc73bf051",
   "metadata": {},
   "outputs": [],
   "source": [
    " net.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebb5aaa8-71e9-4345-9d0d-e364d0f36aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenetv2_1.00_224 (Func  (None, 7, 7, 1280)        2257984   \n",
      " tional)                                                         \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 1280)              0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 120)               153720    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2411704 (9.20 MB)\n",
      "Trainable params: 153720 (600.47 KB)\n",
      "Non-trainable params: 2257984 (8.61 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "188db742-357b-48a2-9419-546ea11412c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 202s 533ms/step - loss: 1.5501 - accuracy: 0.6075 - val_loss: 0.9152 - val_accuracy: 0.7325\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 184s 491ms/step - loss: 0.6050 - accuracy: 0.8223 - val_loss: 0.8447 - val_accuracy: 0.7479\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 196s 522ms/step - loss: 0.4007 - accuracy: 0.8935 - val_loss: 0.8331 - val_accuracy: 0.7549\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 190s 507ms/step - loss: 0.2797 - accuracy: 0.9377 - val_loss: 0.8309 - val_accuracy: 0.7564\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 184s 492ms/step - loss: 0.2019 - accuracy: 0.9647 - val_loss: 0.8350 - val_accuracy: 0.7559\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 185s 493ms/step - loss: 0.1501 - accuracy: 0.9809 - val_loss: 0.8423 - val_accuracy: 0.7583\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 186s 496ms/step - loss: 0.1146 - accuracy: 0.9884 - val_loss: 0.8526 - val_accuracy: 0.7589\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 188s 503ms/step - loss: 0.0895 - accuracy: 0.9922 - val_loss: 0.8652 - val_accuracy: 0.7596\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 193s 514ms/step - loss: 0.0715 - accuracy: 0.9948 - val_loss: 0.8794 - val_accuracy: 0.7594\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 181s 482ms/step - loss: 0.0581 - accuracy: 0.9958 - val_loss: 0.8945 - val_accuracy: 0.7590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f296c286b50>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(\n",
    "        train_batches,\n",
    "        epochs=10,\n",
    "        validation_data=test_batches\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "729c27ff-c5bb-4056-8d3c-ec5255c7733f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9929999709129333"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_accuracy = net.evaluate(train_batches, verbose=0)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56ded716-8611-49e4-a574-2a3df9d980b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save('models/MobileNet.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cf494e-428a-4099-92d0-b3b9e7866b56",
   "metadata": {},
   "source": [
    "## AlexNet-like model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee8cbaaf-f2d5-4664-84d2-6531a43afba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = models.Sequential([\n",
    "        tf.keras.layers.Conv2D(96, 11, activation='relu', strides=(4, 4), padding='valid', input_shape=(224,224,3)),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2)),\n",
    "        tf.keras.layers.Dropout(rate=0.5),\n",
    "        tf.keras.layers.Conv2D(256, 5, activation='relu', strides=(1, 1), padding='same'),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=(2, 2)),\n",
    "        tf.keras.layers.Dropout(rate=0.5),\n",
    "        tf.keras.layers.Conv2D(384, 3, activation='relu', strides=(1, 1), padding='same'),\n",
    "        tf.keras.layers.Conv2D(384, 3, activation='relu', strides=(1, 1), padding='same'),\n",
    "        tf.keras.layers.Conv2D(384, 3, activation='relu', strides=(1, 1), padding='same'),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=(3, 3), strides=2),\n",
    "        tf.keras.layers.Dropout(rate=0.5),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(4096, activation='relu'),\n",
    "        tf.keras.layers.Dense(4096, activation='relu'),\n",
    "        tf.keras.layers.Dense(120, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b696e26-95be-47d5-a243-557a8e23b704",
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e8874fb-12b1-4596-9896-755f214e1cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenetv2_1.00_224 (Func  (None, 7, 7, 1280)        2257984   \n",
      " tional)                                                         \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 1280)              0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 120)               153720    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2411704 (9.20 MB)\n",
      "Trainable params: 153720 (600.47 KB)\n",
      "Non-trainable params: 2257984 (8.61 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee71b887-3517-4704-9b1d-8a3b4a411189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "375/375 [==============================] - 374s 993ms/step - loss: 4.8371 - accuracy: 0.0040 - val_loss: 4.7875 - val_accuracy: 0.0101\n",
      "Epoch 2/10\n",
      "375/375 [==============================] - 362s 964ms/step - loss: 4.7884 - accuracy: 0.0055 - val_loss: 4.7875 - val_accuracy: 0.0101\n",
      "Epoch 3/10\n",
      "375/375 [==============================] - 360s 961ms/step - loss: 4.7884 - accuracy: 0.0056 - val_loss: 4.7875 - val_accuracy: 0.0101\n",
      "Epoch 4/10\n",
      "375/375 [==============================] - 355s 947ms/step - loss: 4.7884 - accuracy: 0.0053 - val_loss: 4.7875 - val_accuracy: 0.0101\n",
      "Epoch 5/10\n",
      "375/375 [==============================] - 362s 967ms/step - loss: 4.7884 - accuracy: 0.0052 - val_loss: 4.7875 - val_accuracy: 0.0101\n",
      "Epoch 6/10\n",
      "375/375 [==============================] - 354s 945ms/step - loss: 4.7884 - accuracy: 0.0049 - val_loss: 4.7875 - val_accuracy: 0.0101\n",
      "Epoch 7/10\n",
      "375/375 [==============================] - 358s 953ms/step - loss: 4.7884 - accuracy: 0.0051 - val_loss: 4.7875 - val_accuracy: 0.0062\n",
      "Epoch 8/10\n",
      "375/375 [==============================] - 360s 960ms/step - loss: 4.7884 - accuracy: 0.0048 - val_loss: 4.7875 - val_accuracy: 0.0062\n",
      "Epoch 9/10\n",
      "375/375 [==============================] - 355s 946ms/step - loss: 4.7884 - accuracy: 0.0048 - val_loss: 4.7875 - val_accuracy: 0.0062\n",
      "Epoch 10/10\n",
      "375/375 [==============================] - 356s 950ms/step - loss: 4.7884 - accuracy: 0.0049 - val_loss: 4.7875 - val_accuracy: 0.0075\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f4f783f2b90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexnet.fit(\n",
    "        train_batches,\n",
    "        epochs=10,\n",
    "        validation_data=test_batches\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed6602ec-2fac-4dde-bacc-3f9b8cdcede3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008333333767950535"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_accuracy = alexnet.evaluate(train_batches, verbose=0)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0417abb5-47c3-43bf-9cc7-2f1b9348629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet.save('models/AlexNet.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
